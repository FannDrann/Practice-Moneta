{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ace22fbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ace22fbf",
        "outputId": "a40140a6-c985-47ed-db9e-07206b7b1eba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2025.4.26)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "! pip3 install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87ace2a6",
      "metadata": {
        "id": "87ace2a6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7c4cbfb",
      "metadata": {
        "id": "b7c4cbfb"
      },
      "outputs": [],
      "source": [
        "MAX_CHUNK_SIZE = 3000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "d7910058",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (5.0.0)\n",
            "Requirement already satisfied: nltk in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.1)\n",
            "Collecting razdel\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (4.52.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (2.7.0+cu118)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (1.7.0)\n",
            "Requirement already satisfied: scipy in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (0.32.0)\n",
            "Requirement already satisfied: Pillow in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: click in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: filelock in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.8.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
            "Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "! pip install sentence-transformers nltk razdel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "284d8df5",
      "metadata": {
        "id": "284d8df5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from razdel import sentenize\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "\n",
        "\n",
        "def get_sentence_embeddings(sentences: list[str], model_name: str = \"FacebookAI/xlm-roberta-base\") -> np.ndarray:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    \n",
        "    embeddings = []\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        # Среднее по токенам как эмбеддинг предложения\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "        embeddings.append(embedding)\n",
        "    return np.array(embeddings)\n",
        "\n",
        "\n",
        "def semantic_chunking(text: str, max_chunk_size: int, similarity_threshold: float, context_window: int) -> list[str]:\n",
        "    # Разделение текста на предложения\n",
        "    sentences = [s.text.strip() for s in sentenize(text) if s.text.strip()]\n",
        "    if not sentences:\n",
        "        return []\n",
        "\n",
        "    # Получение эмбеддингов\n",
        "    embeddings = get_sentence_embeddings(sentences)\n",
        "\n",
        "    # Инициализация чанков\n",
        "    chunks = []\n",
        "    current_chunk = [sentences[0]]\n",
        "    current_size = len(sentences[0])\n",
        "\n",
        "    # Группировка предложений по сходству\n",
        "    for i in range(1, len(sentences)):\n",
        "        # Определение контекстного окна\n",
        "        start_idx = max(0, i - context_window)\n",
        "        end_idx = min(len(sentences), i + context_window + 1)\n",
        "        context_embeddings = embeddings[start_idx:end_idx]\n",
        "        current_embedding = embeddings[i]\n",
        "\n",
        "        # Вычисление косинусного сходства\n",
        "        similarities = cosine_similarity([current_embedding], context_embeddings)[0]\n",
        "        max_similarity = np.max(similarities) if similarities.size > 0 else 0\n",
        "\n",
        "        # Проверка, можно ли добавить предложение в текущий чанк\n",
        "        sentence_size = len(sentences[i])\n",
        "        if current_size + sentence_size <= max_chunk_size and max_similarity >= similarity_threshold:\n",
        "            current_chunk.append(sentences[i])\n",
        "            current_size += sentence_size\n",
        "        else:\n",
        "            # Сохранение текущего чанка и начало нового\n",
        "            chunks.append(\"\\n\\n\".join(current_chunk))\n",
        "            current_chunk = [sentences[i]]\n",
        "            current_size = sentence_size\n",
        "\n",
        "    # Добавление последнего чанка\n",
        "    if current_chunk:\n",
        "        chunks.append(\"\\n\\n\".join(current_chunk))\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "0a24b1ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def process_text(text: str, source: str = \"local_text\") -> None:\n",
        "    \"\"\"Обрабатывает текст, разделяя его на семантические чанки, и сохраняет в JSON-файлы.\"\"\"\n",
        "    # Очистка текста от ненужных маркеров\n",
        "    text = re.sub(r'Страница \\d+ из \\d+', '', text)  # Удаление номеров страниц\n",
        "    text = re.sub(r'г\\. [А-Яа-яЁё-]+, \\d{4}', '', text)  # Удаление города и года\n",
        "    text = text.strip()\n",
        "\n",
        "    # Разделение на чанки\n",
        "    chunks = semantic_chunking(text, MAX_CHUNK_SIZE, 0.80, 2)\n",
        "\n",
        "    # Сохранение чанков в JSON\n",
        "    os.makedirs('./chunks', exist_ok=True)\n",
        "    for i, chunk in enumerate(chunks, 1):\n",
        "        chunk_data = {\n",
        "            \"id\": f\"chunk_{i:02d}\",\n",
        "            \"source\": source,\n",
        "            \"content\": chunk,\n",
        "            \"size\": len(chunk)\n",
        "        }\n",
        "        with open(f\"./chunks/chunk_{i:02d}.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(chunk_data, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "Iis87EPLi5jY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iis87EPLi5jY",
        "outputId": "6dd64a53-32ab-4f32-f29a-9f0756707296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in c:\\users\\ilya\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.0.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "! pip3 install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "143f5324",
      "metadata": {
        "id": "143f5324"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "def read_text_from_pdf(pdf_file_path: str):\n",
        "    if not os.path.exists(pdf_file_path) or not pdf_file_path.lower().endswith(\".pdf\"):\n",
        "        return \"\"\n",
        "    with open(pdf_file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = ''.join(page.extract_text() for page in reader.pages)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b35d1eee",
      "metadata": {
        "id": "b35d1eee"
      },
      "outputs": [],
      "source": [
        "os.makedirs('./chunks', exist_ok=True)\n",
        "text = read_text_from_pdf('monetaoffer.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "bz11lgMtnRtx",
      "metadata": {
        "id": "bz11lgMtnRtx"
      },
      "outputs": [],
      "source": [
        "with open('./res.txt', 'w', encoding=\"utf-8\") as f:\n",
        "    f.write(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "S4Vq76HYoJS2",
      "metadata": {
        "id": "S4Vq76HYoJS2"
      },
      "outputs": [],
      "source": [
        "process_text(text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
